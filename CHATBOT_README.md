# Chatbot IA para Dashboard

## Descripci√≥n

El dashboard ahora incluye un **asistente de IA flotante** que te ayuda a comprender y analizar los datos visualizados. El chatbot utiliza **Ollama** (ejecuci√≥n local de modelos de lenguaje) para proporcionar:

- üìä **Explicaciones de m√©tricas**: Comprende qu√© significa cada KPI (OEE, UPH, scrap%, etc.)
- üí° **Insights sobre datos**: Detecta tendencias, anomal√≠as y patrones interesantes
- ‚ùì **Respuestas a preguntas**: Pregunta lo que quieras sobre los datos actuales
- üéØ **Sugerencias de acciones**: Recibe recomendaciones basadas en el an√°lisis

## Instalaci√≥n de Ollama

### 1. Instalar Ollama

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**macOS:**
```bash
brew install ollama
```

**Windows:**
Descarga desde: https://ollama.ai/download

### 2. Iniciar Ollama

```bash
ollama serve
```

Esto iniciar√° Ollama en `http://localhost:11434`

### 3. Descargar un modelo

Recomendamos comenzar con **Llama 3**:

```bash
ollama pull llama3
```

Otros modelos disponibles:
- `llama3` - Recomendado, buen equilibrio entre calidad y velocidad (4.7GB)
- `mistral` - M√°s r√°pido pero menos potente (4.1GB)
- `llama2` - Versi√≥n anterior, tambi√©n funcional (3.8GB)
- `codellama` - Especializado en c√≥digo (3.8GB)

Puedes ver todos los modelos disponibles en: https://ollama.ai/library

### 4. Verificar instalaci√≥n

```bash
ollama list
```

Deber√≠as ver el modelo descargado listado.

## Uso del Chatbot

### Acceso

1. Ejecuta el dashboard normalmente: `streamlit run app.py`
2. En cualquier p√°gina del dashboard, ver√°s un bot√≥n **üí¨** en la esquina superior derecha
3. Haz clic para abrir el panel del chatbot

### Funcionalidades

El chatbot tiene acceso a **todos los datos del dashboard**, incluyendo:

- **Producci√≥n**: Piezas OK, scrap, m√°quinas, referencias, OEE
- **√ìrdenes**: Planificaci√≥n, avance de √≥rdenes
- **RRHH**: Horas disponibles, absentismo, productividad laboral
- **Almac√©n**: Materia prima, producto terminado, movimientos

### Ejemplos de preguntas

**An√°lisis general:**
- "¬øCu√°l es el estado general de la producci√≥n?"
- "¬øHay alg√∫n problema importante que deba atender?"
- "Dame un resumen de los datos actuales"

**Preguntas espec√≠ficas:**
- "¬øQu√© m√°quina tiene el peor rendimiento?"
- "¬øCu√°l es el porcentaje de scrap de la referencia X?"
- "¬øC√≥mo est√° el absentismo este mes?"
- "¬øQu√© referencias tienen m√°s problemas de calidad?"

**Explicaciones:**
- "Expl√≠came qu√© es el OEE"
- "¬øC√≥mo se calcula el rendimiento?"
- "¬øQu√© significa UPH?"

**Insights y recomendaciones:**
- "¬øHay tendencias preocupantes en los datos?"
- "¬øQu√© acciones me recomiendas tomar?"
- "¬øQu√© deber√≠a mejorar primero?"

### Selector de modelo

En el panel del chatbot puedes cambiar entre los modelos de Ollama que tengas instalados. Cada modelo tiene diferentes caracter√≠sticas:

- **Modelos m√°s grandes** (7B+): Mejores respuestas, m√°s lentos
- **Modelos m√°s peque√±os** (3B-4B): Respuestas m√°s r√°pidas, menos precisas

## Configuraci√≥n avanzada

### Cambiar el puerto de Ollama

Si Ollama est√° ejecut√°ndose en un puerto diferente, puedes modificar la configuraci√≥n en `scripts/dashboard/chatbot.py`:

```python
chatbot = OllamaChatbot(base_url="http://localhost:PUERTO")
```

### Personalizar el modelo por defecto

En `scripts/dashboard/chatbot.py`, l√≠nea 14:

```python
def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama3"):
```

Cambia `"llama3"` por el modelo que prefieras.

## Soluci√≥n de problemas

### "Ollama no est√° disponible"

**Causa:** Ollama no est√° ejecut√°ndose o no est√° accesible.

**Soluci√≥n:**
1. Verifica que Ollama est√© instalado: `ollama --version`
2. Inicia Ollama: `ollama serve`
3. Verifica que est√© escuchando en el puerto correcto: `curl http://localhost:11434/api/tags`

### "No hay modelos disponibles"

**Causa:** No has descargado ning√∫n modelo.

**Soluci√≥n:**
```bash
ollama pull llama3
```

### El chatbot es muy lento

**Causa:** El modelo es demasiado grande para tu hardware.

**Soluci√≥n:**
1. Prueba un modelo m√°s peque√±o: `ollama pull mistral`
2. Cambia al modelo m√°s peque√±o en el selector del chatbot

### El chatbot da respuestas incorrectas

**Causa:** El modelo necesita m√°s contexto o es limitado.

**Soluci√≥n:**
1. Reformula la pregunta de manera m√°s espec√≠fica
2. Prueba con un modelo m√°s potente (llama3 en lugar de mistral)
3. Reinicia la conversaci√≥n con el bot√≥n "üóëÔ∏è Limpiar"

## Arquitectura t√©cnica

### Componentes

1. **OllamaChatbot** (`scripts/dashboard/chatbot.py`):
   - Clase que maneja la comunicaci√≥n con Ollama
   - Construye el contexto con los datos del dashboard
   - Mantiene el historial de conversaci√≥n

2. **render_chatbot_bubble** (`scripts/dashboard/chatbot.py`):
   - Renderiza la UI del chatbot en Streamlit
   - Gestiona el estado de la conversaci√≥n
   - Proporciona la interfaz de usuario

3. **Integraci√≥n en app.py**:
   - El chatbot se renderiza en todas las p√°ginas
   - Tiene acceso a todos los datos cargados

### Flujo de datos

```
Dashboard Data (DataFrames)
        ‚Üì
build_context_prompt() - Resume datos clave
        ‚Üì
User Message + Context
        ‚Üì
Ollama API (localhost:11434)
        ‚Üì
AI Response
        ‚Üì
Display in Chat UI
```

### Privacidad

- **Todos los datos se procesan localmente**: Ollama se ejecuta en tu m√°quina
- **No hay env√≠o a servicios cloud**: A diferencia de ChatGPT/Claude API
- **Control total**: T√∫ controlas qu√© modelos usar y c√≥mo se procesan los datos

## Mejoras futuras

Posibles extensiones del chatbot:

- [ ] Generaci√≥n de gr√°ficos personalizados bajo demanda
- [ ] Exportaci√≥n de insights a PDF/Excel
- [ ] Alertas proactivas basadas en anomal√≠as detectadas
- [ ] Integraci√≥n con notificaciones (email, Slack)
- [ ] Memoria a largo plazo de conversaciones anteriores
- [ ] Fine-tuning del modelo con terminolog√≠a espec√≠fica de tu planta

## Soporte

Si tienes problemas o sugerencias, puedes:

1. Revisar los logs de Ollama: `journalctl -u ollama -f` (Linux)
2. Revisar la documentaci√≥n de Ollama: https://github.com/ollama/ollama
3. Contactar al equipo de desarrollo del dashboard
