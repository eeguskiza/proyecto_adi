#!/bin/bash

# Script de instalaciÃ³n y configuraciÃ³n de Ollama para el chatbot del dashboard
# Compatible con Linux y macOS

set -e

echo "======================================"
echo "  ConfiguraciÃ³n de Ollama para       "
echo "  Dashboard Chatbot                  "
echo "======================================"
echo ""

# FunciÃ³n para detectar el sistema operativo
detect_os() {
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        echo "linux"
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        echo "macos"
    else
        echo "unknown"
    fi
}

OS=$(detect_os)

# Verificar si Ollama ya estÃ¡ instalado
if command -v ollama &> /dev/null; then
    echo "âœ“ Ollama ya estÃ¡ instalado"
    ollama --version
else
    echo "âœ— Ollama no encontrado. Procediendo a instalar..."
    echo ""

    if [ "$OS" == "linux" ]; then
        echo "Instalando Ollama en Linux..."
        curl -fsSL https://ollama.ai/install.sh | sh
    elif [ "$OS" == "macos" ]; then
        echo "Instalando Ollama en macOS..."
        if command -v brew &> /dev/null; then
            brew install ollama
        else
            echo "Homebrew no encontrado. Instalando manualmente..."
            curl -fsSL https://ollama.ai/install.sh | sh
        fi
    else
        echo "Sistema operativo no soportado automÃ¡ticamente."
        echo "Por favor, instala Ollama manualmente desde: https://ollama.ai/download"
        exit 1
    fi
fi

echo ""
echo "======================================"
echo "  Iniciando Ollama                   "
echo "======================================"
echo ""

# Verificar si Ollama estÃ¡ ejecutÃ¡ndose
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âœ“ Ollama ya estÃ¡ ejecutÃ¡ndose en localhost:11434"
else
    echo "Iniciando servicio de Ollama..."

    if [ "$OS" == "linux" ]; then
        # En Linux, usar systemd si estÃ¡ disponible
        if command -v systemctl &> /dev/null; then
            sudo systemctl start ollama
            echo "âœ“ Ollama iniciado como servicio systemd"
        else
            # Si no hay systemd, iniciar en background
            nohup ollama serve > /tmp/ollama.log 2>&1 &
            echo "âœ“ Ollama iniciado en segundo plano (log: /tmp/ollama.log)"
        fi
    elif [ "$OS" == "macos" ]; then
        # En macOS, iniciar en background
        nohup ollama serve > /tmp/ollama.log 2>&1 &
        echo "âœ“ Ollama iniciado en segundo plano (log: /tmp/ollama.log)"
    fi

    # Esperar a que Ollama estÃ© listo
    echo "Esperando a que Ollama estÃ© listo..."
    for i in {1..10}; do
        if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
            echo "âœ“ Ollama estÃ¡ listo"
            break
        fi
        echo "  Intentando conectar... ($i/10)"
        sleep 2
    done
fi

echo ""
echo "======================================"
echo "  Descargando modelo de IA           "
echo "======================================"
echo ""

# Verificar si ya hay modelos descargados
MODELS=$(ollama list 2>/dev/null | tail -n +2 | wc -l)

if [ "$MODELS" -gt 0 ]; then
    echo "Modelos ya descargados:"
    ollama list
    echo ""
    read -p "Â¿Deseas descargar un modelo adicional? (s/N): " download_more

    if [[ ! "$download_more" =~ ^[Ss]$ ]]; then
        echo "âœ“ Usando modelos existentes"
    else
        echo ""
        echo "Modelos recomendados:"
        echo "  1) llama3 (Recomendado, 4.7GB) - Mejor equilibrio calidad/velocidad"
        echo "  2) mistral (4.1GB) - MÃ¡s rÃ¡pido, buena calidad"
        echo "  3) llama2 (3.8GB) - VersiÃ³n anterior, funcional"
        echo "  4) codellama (3.8GB) - Especializado en cÃ³digo"
        echo ""
        read -p "Selecciona un modelo (1-4): " model_choice

        case $model_choice in
            1) MODEL="llama3" ;;
            2) MODEL="mistral" ;;
            3) MODEL="llama2" ;;
            4) MODEL="codellama" ;;
            *) MODEL="llama3" ;;
        esac

        echo "Descargando $MODEL (esto puede tardar varios minutos)..."
        ollama pull $MODEL
        echo "âœ“ Modelo $MODEL descargado"
    fi
else
    echo "No se encontraron modelos. Descargando llama3 (recomendado)..."
    echo "Esto descargarÃ¡ aproximadamente 4.7GB. Puede tardar varios minutos."
    echo ""
    read -p "Â¿Continuar? (S/n): " continue_download

    if [[ "$continue_download" =~ ^[Nn]$ ]]; then
        echo "Descarga cancelada. Puedes descargar manualmente mÃ¡s tarde con:"
        echo "  ollama pull llama3"
    else
        ollama pull llama3
        echo "âœ“ Modelo llama3 descargado"
    fi
fi

echo ""
echo "======================================"
echo "  ConfiguraciÃ³n completada            "
echo "======================================"
echo ""
echo "âœ“ Todo listo para usar el chatbot del dashboard"
echo ""
echo "PrÃ³ximos pasos:"
echo "  1. Ejecuta el dashboard: streamlit run app.py"
echo "  2. Haz clic en el botÃ³n ðŸ’¬ para abrir el chatbot"
echo "  3. Â¡Empieza a hacer preguntas sobre tus datos!"
echo ""
echo "Comandos Ãºtiles:"
echo "  - Ver modelos: ollama list"
echo "  - Descargar modelo: ollama pull <nombre>"
echo "  - Detener Ollama: killall ollama (o systemctl stop ollama en Linux)"
echo ""
echo "DocumentaciÃ³n completa: CHATBOT_README.md"
echo "======================================"
