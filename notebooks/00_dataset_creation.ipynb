{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de72411a",
   "metadata": {},
   "source": [
    "#### ***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1801d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloque 0: imports y opciones ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0d9e8",
   "metadata": {},
   "source": [
    "#### ***Rutas de ficheros***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bec090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloque 1: rutas ---\n",
    "PATH_RRHH         = \"rrhh_turno.csv\"              # año_mes, horas_teoricas, reduccion_tco, horas_ajustadas, horas_enfermedad, horas_accidente, horas_permiso, horas_netas\n",
    "PATH_INGENIERIA   = \"datos_referencias.csv\"       # ref_id, familia, peso_neto_kg\n",
    "PATH_MOV_ALMACEN  = \"almacen_movimientos.csv\"     # mov_id, material_lot_id, item_ref_id, tipo_mov, qty, fecha_ts\n",
    "PATH_COMPRAS      = \"compras_lotes.csv\"           # material_lot_id, laminado_id, ref_materia, qty_recibida, udm, peso_bruto, uds, fecha_recepcion_ts\n",
    "PATH_WORK_ORDERS  = \"ordenes_header.csv\"          # work_order_id, ref_id, familia, cliente, qty_plan, fecha_lanzamiento, due_date, planta_inicio\n",
    "PATH_HIST_OPS     = \"produccion_operaciones.csv\"  # work_order_id,work_order_id_raw,op_id,machine_id,machine_name,planta,op_text,ref_id,ref_id_raw,ts_ini,ts_fin,duracion_min,piezas_ok,piezas_scrap,evento,tipo_incidencia,operario_id,operario_nombre,of_impute_src,ref_impute_src,origen_fichero,origen_ruta,material_lot_id\n",
    "OUTPUT_PATH       = \"dataset_integrado_2025.csv\"  # salida integrada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc947089",
   "metadata": {},
   "source": [
    "#### ***Limpieza y Helpers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6e4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloque 2: helpers de limpieza ---\n",
    "\n",
    "def norm_ref(x, width=6):\n",
    "    \"\"\"\n",
    "    Normaliza referencias:\n",
    "    - Acepta strings, ints, floats o NaN.\n",
    "    - Devuelve string left-pad con ceros a 'width' (por defecto 6).\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    # casos tipo 081901, '081901', 81901.0, 81000.0\n",
    "    try:\n",
    "        # Si viene como '000012', respeta\n",
    "        s = str(x).strip()\n",
    "        # Quita '.0' si es float representado como string\n",
    "        if s.endswith(\".0\"):\n",
    "            s = s[:-2]\n",
    "        # Si contiene no-dígitos, elimina espacios y vuelve a probar\n",
    "        if not s.isdigit():\n",
    "            # intenta como float->int\n",
    "            v = int(round(float(s)))\n",
    "            return str(v).zfill(width)\n",
    "        return s.zfill(width)\n",
    "    except:\n",
    "        try:\n",
    "            v = int(round(float(x)))\n",
    "            return str(v).zfill(width)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "def to_month_str(ts):\n",
    "    \"\"\"Convierte timestamp a 'YYYY-MM' string.\"\"\"\n",
    "    if pd.isna(ts):\n",
    "        return np.nan\n",
    "    return f\"{ts.year:04d}-{ts.month:02d}\"\n",
    "\n",
    "def coerce_dt(s):\n",
    "    \"\"\"to_datetime tolerante, NaT si no se puede (e.g., 'PENDING').\"\"\"\n",
    "    return pd.to_datetime(s, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0a805",
   "metadata": {},
   "source": [
    "#### ***Carga CSV's***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a19b54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloque 3: carga segura de CSVs ---\n",
    "\n",
    "rrhh = pd.read_csv(PATH_RRHH, dtype=str)\n",
    "ing  = pd.read_csv(PATH_INGENIERIA, dtype=str)\n",
    "movs = pd.read_csv(PATH_MOV_ALMACEN, dtype=str)\n",
    "compr= pd.read_csv(PATH_COMPRAS, dtype=str)\n",
    "wo   = pd.read_csv(PATH_WORK_ORDERS, dtype=str)\n",
    "ops  = pd.read_csv(PATH_HIST_OPS, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0398a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRHH columnas detectadas: ['año_mes', 'horas_teoricas', 'reduccion_tco', 'horas_ajustadas', 'horas_enfermedad', 'horas_accidente', 'horas_permiso', 'horas_netas']\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque 4.1: RRHH mensual (robusto) ---\n",
    "# Limpieza de cabeceras: espacios, BOM, guiones, mayúsculas\n",
    "rrhh.columns = (rrhh.columns\n",
    "                .str.strip()\n",
    "                .str.replace('\\ufeff','', regex=False)\n",
    "                .str.replace(' ', '_')\n",
    "                .str.replace('-', '_')\n",
    "                .str.replace(r'[^0-9A-Za-z_ñáéíóúÑÁÉÍÓÚ]', '', regex=True)\n",
    "                .str.lower())\n",
    "\n",
    "# Normaliza variantes comunes\n",
    "rrhh = rrhh.rename(columns={\n",
    "    'ano_mes': 'año_mes',\n",
    "    'anio_mes': 'año_mes',\n",
    "    'mes_anio': 'año_mes',\n",
    "    'horas_teoricas_hrs': 'horas_teoricas',\n",
    "    'horas_teoricashrs': 'horas_teoricas',\n",
    "})\n",
    "\n",
    "expected = [\n",
    "    \"año_mes\",\"horas_teoricas\",\"reduccion_tco\",\"horas_ajustadas\",\n",
    "    \"horas_enfermedad\",\"horas_accidente\",\"horas_permiso\",\"horas_netas\"\n",
    "]\n",
    "\n",
    "# Crea columnas faltantes para evitar KeyError\n",
    "for c in expected:\n",
    "    if c not in rrhh.columns:\n",
    "        rrhh[c] = np.nan\n",
    "\n",
    "# Tipos\n",
    "rrhh[\"año_mes\"] = rrhh[\"año_mes\"].astype(str).str.strip()\n",
    "for c in [\"horas_teoricas\",\"reduccion_tco\",\"horas_ajustadas\",\"horas_enfermedad\",\"horas_accidente\",\"horas_permiso\",\"horas_netas\"]:\n",
    "    rrhh[c] = pd.to_numeric(rrhh[c], errors=\"coerce\")\n",
    "\n",
    "rrhh = rrhh.drop_duplicates(subset=[\"año_mes\"])\n",
    "\n",
    "print(\"RRHH columnas detectadas:\", list(rrhh.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be12912",
   "metadata": {},
   "source": [
    "#### ***Limpieza por tabla***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136ead53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4083/2875816976.py:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  wo[\"planta_inicio\"] = wo[\"planta_inicio\"].replace({\"PENDING\":np.nan}).astype(\"string\")\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque 4.1: RRHH mensual ---\n",
    "rrhh[\"año_mes\"] = rrhh[\"año_mes\"].astype(str).str.strip()\n",
    "cols_num = [\"horas_teoricas\",\"reduccion_tco\",\"horas_ajustadas\",\"horas_enfermedad\",\"horas_accidente\",\"horas_permiso\",\"horas_netas\"]\n",
    "for c in cols_num:\n",
    "    rrhh[c] = pd.to_numeric(rrhh[c], errors=\"coerce\")\n",
    "rrhh = rrhh.drop_duplicates(subset=[\"año_mes\"])\n",
    "\n",
    "# --- Bloque 4.2: Ingeniería (maestro de piezas) ---\n",
    "ing[\"ref_id_str\"] = ing[\"ref_id\"].map(norm_ref)\n",
    "ing[\"familia\"] = ing[\"familia\"].astype(str).str.strip()\n",
    "ing[\"peso_neto_kg\"] = pd.to_numeric(ing[\"peso_neto_kg\"], errors=\"coerce\")\n",
    "\n",
    "# --- Bloque 4.3: Movimientos a almacén (solo IN) ---\n",
    "movs[\"fecha_ts\"] = coerce_dt(movs[\"fecha_ts\"])\n",
    "movs = movs[movs[\"tipo_mov\"].str.upper().eq(\"IN\")]\n",
    "movs[\"item_ref_id_str\"] = movs[\"item_ref_id\"].map(norm_ref)\n",
    "movs[\"qty\"] = pd.to_numeric(movs[\"qty\"], errors=\"coerce\").fillna(0)\n",
    "movs[\"fecha\"] = movs[\"fecha_ts\"].dt.normalize()  # fecha sin hora\n",
    "mov_aggr = (movs\n",
    "            .groupby([\"item_ref_id_str\",\"fecha\"], as_index=False)[\"qty\"]\n",
    "            .sum()\n",
    "            .rename(columns={\"qty\":\"qty_in_almacen_dia\"}))\n",
    "\n",
    "# --- Bloque 4.4: Compras de materia prima (lotes) ---\n",
    "compr[\"fecha_recepcion_ts\"] = coerce_dt(compr[\"fecha_recepcion_ts\"])\n",
    "compr[\"material_lot_id\"] = compr[\"material_lot_id\"].astype(str).str.strip()\n",
    "compr[\"ref_materia_str\"] = compr[\"ref_materia\"].map(norm_ref)\n",
    "for c in [\"qty_recibida\",\"peso_bruto\",\"uds\"]:\n",
    "    if c in compr:\n",
    "        compr[c] = pd.to_numeric(compr[c], errors=\"coerce\")\n",
    "\n",
    "# --- Bloque 4.5: Work orders ---\n",
    "wo[\"work_order_id\"] = wo[\"work_order_id\"].astype(str).str.strip()\n",
    "wo[\"ref_id_str\"] = wo[\"ref_id\"].map(norm_ref)\n",
    "wo[\"qty_plan\"] = pd.to_numeric(wo[\"qty_plan\"], errors=\"coerce\")\n",
    "for c in [\"fecha_lanzamiento\",\"due_date\"]:\n",
    "    wo[c] = coerce_dt(wo[c])\n",
    "wo[\"planta_inicio\"] = wo[\"planta_inicio\"].replace({\"PENDING\":np.nan}).astype(\"string\")\n",
    "\n",
    "# --- Bloque 4.6: Histórico de operaciones ---\n",
    "ops[\"ts_ini\"] = coerce_dt(ops[\"ts_ini\"])\n",
    "ops[\"ts_fin\"] = coerce_dt(ops[\"ts_fin\"])\n",
    "ops[\"duracion_min\"] = pd.to_numeric(ops[\"duracion_min\"], errors=\"coerce\")\n",
    "ops[\"piezas_ok\"] = pd.to_numeric(ops[\"piezas_ok\"], errors=\"coerce\").fillna(0)\n",
    "ops[\"piezas_scrap\"] = pd.to_numeric(ops[\"piezas_scrap\"], errors=\"coerce\").fillna(0)\n",
    "# ref preferente: 'ref_id' si existe, si no 'ref_id_raw'\n",
    "ops[\"ref_id_str\"] = np.where(ops.get(\"ref_id\").notna(), ops[\"ref_id\"].map(norm_ref), ops.get(\"ref_id_raw\").map(norm_ref))\n",
    "# fecha de fin para agregados y joins diarios\n",
    "ops[\"fecha\"] = ops[\"ts_fin\"].dt.normalize()\n",
    "# downtime solo cuando evento=Incidencia\n",
    "ops[\"evento\"] = ops[\"evento\"].astype(str)\n",
    "ops[\"downtime_min\"] = np.where(ops[\"evento\"].str.lower().str.contains(\"incidencia\"), ops[\"duracion_min\"], 0).astype(float)\n",
    "# material_lot_id a string y 0 -> NaN\n",
    "if \"material_lot_id\" in ops:\n",
    "    ops[\"material_lot_id\"] = ops[\"material_lot_id\"].astype(str).str.strip()\n",
    "    ops.loc[ops[\"material_lot_id\"].isin([\"0\",\"0.0\",\"nan\",\"None\",\"\"]), \"material_lot_id\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f80b13",
   "metadata": {},
   "source": [
    "#### ***Unión Base Sobre Operaciones***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "292daae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloque 5: joins principales sobre ops ---\n",
    "\n",
    "# 5.1 Join Ingeniería (familia, peso)\n",
    "ops1 = ops.merge(ing[[\"ref_id_str\",\"familia\",\"peso_neto_kg\"]], on=\"ref_id_str\", how=\"left\")\n",
    "\n",
    "# 5.2 Join Work Orders (cliente, qty_plan, due_date, planta_inicio)\n",
    "ops2 = ops1.merge(\n",
    "    wo[[\"work_order_id\",\"ref_id_str\",\"cliente\",\"qty_plan\",\"fecha_lanzamiento\",\"due_date\",\"planta_inicio\"]],\n",
    "    on=[\"work_order_id\",\"ref_id_str\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5.3 Join Compras por material_lot_id (si existe)\n",
    "if \"material_lot_id\" in ops2.columns:\n",
    "    ops3 = ops2.merge(\n",
    "        compr[[\"material_lot_id\",\"ref_materia_str\",\"qty_recibida\",\"peso_bruto\",\"uds\",\"fecha_recepcion_ts\"]],\n",
    "        on=\"material_lot_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "else:\n",
    "    ops3 = ops2.copy()\n",
    "\n",
    "# 5.4 Join Movimientos IN agregados por ref y fecha (qty_in_almacen_dia)\n",
    "ops4 = ops3.merge(\n",
    "    mov_aggr.rename(columns={\"item_ref_id_str\":\"ref_id_str\"}),\n",
    "    on=[\"ref_id_str\",\"fecha\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 5.5 Join RRHH por mes de ts_fin (global, sin planta)\n",
    "ops4[\"año_mes\"] = ops4[\"ts_fin\"].dt.strftime(\"%Y-%m\")\n",
    "ops5 = ops4.merge(rrhh, on=\"año_mes\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819f334",
   "metadata": {},
   "source": [
    "#### ***Métricas Derivadas***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598fc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloque 6: features derivadas fáciles ---\n",
    "\n",
    "total_pzs = ops5[\"piezas_ok\"] + ops5[\"piezas_scrap\"]\n",
    "ops5[\"throughput_uph\"] = np.where(ops5[\"duracion_min\"]>0, 60.0 * ops5[\"piezas_ok\"] / ops5[\"duracion_min\"], np.nan)\n",
    "ops5[\"scrap_rate\"] = np.where(total_pzs>0, ops5[\"piezas_scrap\"] / total_pzs, np.nan)\n",
    "ops5[\"consumo_materia_kg\"] = ops5[\"peso_neto_kg\"] * ops5[\"piezas_ok\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8229fbc",
   "metadata": {},
   "source": [
    "#### ***Lead time a stock (fin de operación → siguiente entrada IN)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f5670bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4083/406989896.py:41: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  lead_series = ends.groupby(\"ref_id_str\", group_keys=False).apply(compute_lead_for_ref)\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque 7: lead_time_al_almacen_dias (robusto, por ref) ---\n",
    "\n",
    "# 1) Preparar tablas de fechas\n",
    "ends = ops5[[\"ref_id_str\",\"ts_fin\"]].dropna().copy()\n",
    "ends[\"end_date\"] = ends[\"ts_fin\"].dt.normalize()\n",
    "ends = ends.dropna(subset=[\"ref_id_str\",\"end_date\"])\n",
    "\n",
    "mov_dates = mov_aggr[[\"item_ref_id_str\",\"fecha\"]].drop_duplicates().rename(columns={\"item_ref_id_str\":\"ref_id_str\"})\n",
    "mov_dates = mov_dates.dropna(subset=[\"ref_id_str\",\"fecha\"])\n",
    "\n",
    "# Garantizar tipos datetime\n",
    "ends[\"end_date\"] = pd.to_datetime(ends[\"end_date\"])\n",
    "mov_dates[\"fecha\"] = pd.to_datetime(mov_dates[\"fecha\"])\n",
    "\n",
    "if mov_dates.empty:\n",
    "    # Sin movimientos: no se puede calcular lead time\n",
    "    ops5[\"end_date\"] = ops5[\"ts_fin\"].dt.normalize()\n",
    "    ops5[\"lead_time_al_almacen_dias\"] = np.nan\n",
    "else:\n",
    "    # 2) Índice de movimientos por ref, ordenado\n",
    "    mov_group = {k: v.sort_values(\"fecha\")[\"fecha\"].values\n",
    "                 for k, v in mov_dates.groupby(\"ref_id_str\")}\n",
    "\n",
    "    # 3) Para cada referencia, primer movimiento >= end_date\n",
    "    def compute_lead_for_ref(sub):\n",
    "        ref = sub.name\n",
    "        end_vals = sub[\"end_date\"].values\n",
    "        stock_vals = mov_group.get(ref, None)\n",
    "        if stock_vals is None or len(stock_vals) == 0:\n",
    "            return pd.Series([np.nan]*len(sub), index=sub.index)\n",
    "        idx = np.searchsorted(stock_vals, end_vals, side=\"left\")\n",
    "        chosen = []\n",
    "        for i in idx:\n",
    "            if i >= len(stock_vals):\n",
    "                chosen.append(pd.NaT)\n",
    "            else:\n",
    "                chosen.append(stock_vals[i])\n",
    "        chosen = pd.to_datetime(chosen)\n",
    "        return (chosen - sub[\"end_date\"]).dt.days.astype(\"float\")\n",
    "\n",
    "    lead_series = ends.groupby(\"ref_id_str\", group_keys=False).apply(compute_lead_for_ref)\n",
    "    ends[\"lead_time_al_almacen_dias\"] = lead_series.values\n",
    "\n",
    "    # 4) Mapear de vuelta\n",
    "    key = ends[\"ref_id_str\"].astype(str) + \"|\" + ends[\"end_date\"].astype(str)\n",
    "    lead_map = dict(zip(key, ends[\"lead_time_al_almacen_dias\"]))\n",
    "\n",
    "    ops5[\"end_date\"] = ops5[\"ts_fin\"].dt.normalize()\n",
    "    ops5[\"__k\"] = ops5[\"ref_id_str\"].astype(str) + \"|\" + ops5[\"end_date\"].astype(str)\n",
    "    ops5[\"lead_time_al_almacen_dias\"] = ops5[\"__k\"].map(lead_map)\n",
    "    ops5 = ops5.drop(columns=[\"__k\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e1662",
   "metadata": {},
   "source": [
    "#### ***Fusión Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16a70d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloque 8: orden final de columnas útiles ---\n",
    "\n",
    "cols_final = [\n",
    "    # claves y tiempo\n",
    "    \"work_order_id\",\"op_id\",\"machine_id\",\"machine_name\",\"planta\",\"op_text\",\n",
    "    \"ref_id_str\",\"familia\",\"peso_neto_kg\",\"material_lot_id\",\"ref_materia_str\",\n",
    "    \"ts_ini\",\"ts_fin\",\"fecha\",\"duracion_min\",\"evento\",\"tipo_incidencia\",\n",
    "    # cantidades\n",
    "    \"piezas_ok\",\"piezas_scrap\",\"qty_plan\",\"qty_in_almacen_dia\",\n",
    "    # rrhh\n",
    "    \"año_mes\",\"horas_teoricas\",\"reduccion_tco\",\"horas_ajustadas\",\"horas_enfermedad\",\"horas_accidente\",\"horas_permiso\",\"horas_netas\",\n",
    "    # compras\n",
    "    \"qty_recibida\",\"peso_bruto\",\"uds\",\"fecha_recepcion_ts\",\n",
    "    # derivados\n",
    "    \"throughput_uph\",\"scrap_rate\",\"downtime_min\",\"consumo_materia_kg\",\"lead_time_al_almacen_dias\",\n",
    "]\n",
    "\n",
    "# Mantén solo las que existan\n",
    "cols_final = [c for c in cols_final if c in ops5.columns]\n",
    "df = ops5[cols_final].copy()\n",
    "\n",
    "# Orden y tipos suaves\n",
    "num_cols = [\"duracion_min\",\"piezas_ok\",\"piezas_scrap\",\"qty_plan\",\"qty_in_almacen_dia\",\n",
    "            \"horas_teoricas\",\"reduccion_tco\",\"horas_ajustadas\",\"horas_enfermedad\",\"horas_accidente\",\"horas_permiso\",\"horas_netas\",\n",
    "            \"qty_recibida\",\"peso_bruto\",\"uds\",\"throughput_uph\",\"scrap_rate\",\"downtime_min\",\"consumo_materia_kg\",\"peso_neto_kg\",\"lead_time_al_almacen_dias\"]\n",
    "for c in num_cols:\n",
    "    if c in df:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea799ca3",
   "metadata": {},
   "source": [
    "#### ***Tests Finales***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd4c228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas integradas: 362433\n",
      "Refs distintas: 91\n",
      "Máquinas distintas: 78\n",
      "  work_order_id         op_id machine_id machine_name    planta       op_text ref_id_str             familia  peso_neto_kg material_lot_id ref_materia_str  \\\n",
      "0       24/0658  SOLDADURA-RE        515    SOLDADORA  Zaldibar  SOLDADURA-RE     081000  CORONA DE ARRANQUE          1.59             NaN             NaN   \n",
      "1       24/0658  SOLDADURA-RE        515    SOLDADORA  Zaldibar  SOLDADURA-RE     081000  CORONA DE ARRANQUE          1.59             NaN             NaN   \n",
      "2       24/0674   RECTIFICADO       1001  Linea Luk 1   Abadiño   RECTIFICADO     124203                 NaN           NaN             NaN             NaN   \n",
      "\n",
      "               ts_ini              ts_fin      fecha  duracion_min      evento  tipo_incidencia  piezas_ok  piezas_scrap  qty_plan  qty_in_almacen_dia  \\\n",
      "0 2025-01-06 21:47:00 2025-01-06 21:50:00 2025-01-06           3.0  Producción              NaN          0             0    4008.0                 NaN   \n",
      "1 2025-01-06 21:48:00 2025-01-06 21:50:00 2025-01-06           2.0  Incidencia  CALENTAR HIDRÁU          0             0    4008.0                 NaN   \n",
      "2 2025-01-06 21:49:00 2025-01-07 06:09:00 2025-01-07         500.0  Producción              NaN        322             3       NaN               360.0   \n",
      "\n",
      "   año_mes  horas_teoricas  reduccion_tco  horas_ajustadas  horas_enfermedad  horas_accidente  horas_permiso  horas_netas  qty_recibida  peso_bruto  uds  \\\n",
      "0  2025-01         12350.0          788.0          11562.0             752.0              0.0          390.0      10420.0           NaN         NaN  NaN   \n",
      "1  2025-01         12350.0          788.0          11562.0             752.0              0.0          390.0      10420.0           NaN         NaN  NaN   \n",
      "2  2025-01         12350.0          788.0          11562.0             752.0              0.0          390.0      10420.0           NaN         NaN  NaN   \n",
      "\n",
      "  fecha_recepcion_ts  throughput_uph  scrap_rate  downtime_min  consumo_materia_kg  lead_time_al_almacen_dias  \n",
      "0                NaT            0.00         NaN           0.0                 0.0                        1.0  \n",
      "1                NaT            0.00         NaN           2.0                 0.0                        1.0  \n",
      "2                NaT           38.64    0.009231           0.0                 NaN                        3.0  \n"
     ]
    }
   ],
   "source": [
    "# --- Bloque 9: sanity checks mínimos ---\n",
    "print(\"Filas integradas:\", len(df))\n",
    "print(\"Refs distintas:\", df[\"ref_id_str\"].nunique() if \"ref_id_str\" in df else \"-\")\n",
    "print(\"Máquinas distintas:\", df[\"machine_name\"].nunique() if \"machine_name\" in df else \"-\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86059e",
   "metadata": {},
   "source": [
    "#### ***Guardado Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9aee043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado en: dataset_integrado_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque 10: export ---\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"Guardado en:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b9bab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Máquinas distintas: 84\n",
      "       machine_id         machine_name   planta\n",
      "195210        134         Curvadora134  Abadiño\n",
      "1390           25          Curvadora25  Abadiño\n",
      "1640          122    Fresadora Hey 122  Abadiño\n",
      "1650           12          Fresadora12  Abadiño\n",
      "5628          140         Fresadora140  Abadiño\n",
      "29423         141         Fresadora141  Abadiño\n",
      "1619           80          Fresadora80  Abadiño\n",
      "1503          110         Granalladora  Abadiño\n",
      "11            517  Granalladora Sthick  Abadiño\n",
      "1626          118                Horno  Abadiño\n",
      "7             519         Horno Arrola  Abadiño\n",
      "2            1001          Linea Luk 1  Abadiño\n",
      "16           1002          Linea Luk 2  Abadiño\n",
      "108          1003          Linea Luk 3  Abadiño\n",
      "1641         4001          Linea SACHS  Abadiño\n",
      "5187         3001     Linea Volkswagen  Abadiño\n",
      "436            32               PRENSA  Abadiño\n",
      "1399           31               PRENSA  Abadiño\n",
      "2966           27             Prensa27  Abadiño\n",
      "20230          95           Prensa95-1  Abadiño\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque 11: listado de máquinas y top-5 por recurso ---\n",
    "\n",
    "# 11.1 Máquinas distintas\n",
    "maquinas = (df[[\"machine_id\",\"machine_name\",\"planta\"]]\n",
    "            .drop_duplicates()\n",
    "            .sort_values([\"planta\",\"machine_name\"], na_position=\"last\"))\n",
    "print(\"Máquinas distintas:\", len(maquinas))\n",
    "print(maquinas.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b90c9106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OF->ref resumen (primeras 10 filas):\n",
      "   work_order_id ref_id_str              familia  piezas_ok  piezas_scrap  qty_plan   total  scrap_rate  cumplimiento\n",
      "0        22/1009     003501   CORONA DE ARRANQUE        868            31     110.0     899    0.034483      7.890909\n",
      "1        23/0172     191000   CORONA DE ARRANQUE          0             0       NaN       0         NaN           NaN\n",
      "2        23/0366     818003  INKREMENT GEBERRING       9466            14       NaN    9480    0.001477           NaN\n",
      "3        23/0378     010002   CORONA DE ARRANQUE        556            16       NaN     572    0.027972           NaN\n",
      "4        23/0554     002260   CORONA DE ARRANQUE         60             0       NaN      60    0.000000           NaN\n",
      "..           ...        ...                  ...        ...           ...       ...     ...         ...           ...\n",
      "95       25/0049     904802   CORONA DE ARRANQUE     109380            92    4068.0  109472    0.000840     26.887906\n",
      "96       25/0051     081901   CORONA DE ARRANQUE      29317            26    2591.0   29343    0.000886     11.314936\n",
      "97       25/0052     563404   CORONA DE ARRANQUE      75026           109       NaN   75135    0.001451           NaN\n",
      "98       25/0055     009400   CORONA DE ARRANQUE      90805           110    2200.0   90915    0.001210     41.275000\n",
      "99       25/0056     190301   CORONA DE ARRANQUE      68922           114    2702.0   69036    0.001651     25.507772\n",
      "\n",
      "[100 rows x 9 columns]\n",
      "\n",
      "Detalle para OF 22/1009:\n",
      "  work_order_id ref_id_str             familia  piezas_ok  piezas_scrap  total  scrap_rate  qty_plan  cumplimiento\n",
      "0       22/1009     003501  CORONA DE ARRANQUE        868            31    899    0.034483     110.0      7.890909\n"
     ]
    }
   ],
   "source": [
    "# --- Bloque 12: resumen jerárquico OF -> ref -> piezas ---\n",
    "\n",
    "# 12.1 Agregado principal\n",
    "of_ref = (df.groupby([\"work_order_id\",\"ref_id_str\",\"familia\"], as_index=False)\n",
    "            .agg(piezas_ok=(\"piezas_ok\",\"sum\"),\n",
    "                 piezas_scrap=(\"piezas_scrap\",\"sum\"),\n",
    "                 qty_plan=(\"qty_plan\",\"max\"))\n",
    "         )\n",
    "of_ref[\"total\"] = of_ref[\"piezas_ok\"] + of_ref[\"piezas_scrap\"]\n",
    "of_ref[\"scrap_rate\"] = np.where(of_ref[\"total\"]>0, of_ref[\"piezas_scrap\"]/of_ref[\"total\"], np.nan)\n",
    "of_ref[\"cumplimiento\"] = np.where(of_ref[\"qty_plan\"].notna() & (of_ref[\"qty_plan\"]>0),\n",
    "                                  of_ref[\"piezas_ok\"]/of_ref[\"qty_plan\"], np.nan)\n",
    "\n",
    "print(\"OF->ref resumen (primeras 10 filas):\")\n",
    "print(of_ref.head(100))\n",
    "\n",
    "# 12.2 Vista rápida por OF concreto (cambia el ID)\n",
    "OF_EJEMPLO = of_ref[\"work_order_id\"].dropna().unique()[0] if len(of_ref)>0 else None\n",
    "if OF_EJEMPLO:\n",
    "    print(f\"\\nDetalle para OF {OF_EJEMPLO}:\")\n",
    "    print(of_ref[of_ref[\"work_order_id\"]==OF_EJEMPLO]\n",
    "          .sort_values(\"piezas_ok\", ascending=False)\n",
    "          [[\"work_order_id\",\"ref_id_str\",\"familia\",\"piezas_ok\",\"piezas_scrap\",\"total\",\"scrap_rate\",\"qty_plan\",\"cumplimiento\"]]\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
